---
title: Is Attention All You Need? ğŸ‘€
description: The answer is yes - attention is all you need. In this blog post, we explore the famous Transformer architecture and why is it  that attention is all you need. ğŸ¤”
author: Ludek Cizinsky
published: 02/01/2023

image: /public/posts/transformers-headline.png
layout: ../../layouts/BlogLayout.astro
---

As of writing this blog post, it has been 6 years since the `Transformers` architecture has been [proposed](https://arxiv.org/pdf/1706.03762.pdf) and took the AI field by storm. Not only it shaped the future of the initially intended field  of NLP, but it impacted many other domains from computer vision to audio intelligence. In this blog post, we look under the hood of the `Transformer` architecture and explain what makes it so powerful. Without further ado, let's get started! ğŸ“

### ğŸ“– Before Transformers

---

Before we start talking about `Transformers`, we need to understand its predecessor - `Reccurent Neural Networks`. 
