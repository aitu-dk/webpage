---
import Layout from "../layouts/Layout.astro";
---

<Layout title="Schedule" description="Schedule for the current school year">
  <main>
    <div class="max-w-4xl mt-10">
      <a href="/" class="text-sm text-accent">Back</a>
      <h1 class="my-6 text-3xl font-medium sm:text-4xl">
        Preliminary Schedule Spring 2025
      </h1>

<p class="text-lg font-light text-secondary my-2">
      AITU is an open-for-all study group where interested people come together to share insights and discuss recent advencents in AI and general topics within the field of Machine Learning. One of our members always gives an introduction to the topic after which an open discussion follows. We meet in 2F13 (ITU DR Building, Kaj Munks Vej 11) each Wednesday at 6pm over snacks and pizza and finish around 8pm.
      </p>
<p class="text-lg font-light text-secondary my-2">
This semester we would like to strenghten our understaing about where the current revolution in AI started - Statistical Learning. Therefore, we will start by revisiting some topics in the area.
      </p>
      <p class="text-lg font-light text-secondary my-2">
        Overall, you don’t have to read papers and the covered material before
        the meeting, but it is encouraged to have a look. You will get much more
        out of the session and participate in the open discussion actively,
        which is fun in itself!
      </p>
      
      <h2 class="my-4 text-2xl font-medium">Statistical Learning Revisited</h2>
      <p class="text-lg font-light text-secondary my-2">
        To start off, we will conduct a grand comparison of historical classification models, such as LDA/QDA, SVMs and Decision Trees as a Kahoot quiz. We will look at their performance in different dimensionality settings and discuss their respective strong and weak sides.
      </p>
      <p class="text-lg font-light text-secondary my-2">
        During our second meeting we will look at a paper <a href="https://x.com/GaelVaroquaux/status/1549422403889106944" class="text-accent">that received some attention on Twitter</a> (X) in 2022, a couple of months before ChatGPT was released. Does it still hold? Is there still space in the world dominated by deep neural networks for tree-based models?
      </p>
      <p class="text-lg font-light text-secondary my-2">
        We will continue the discussion with <s>Catboost</s> FT-Transformer, and later explore additional topics like GMMs and MCMC.
      </p>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W8: A Comparison of Historical Classification Models<br />
          <div class="text-sm ml-4">
            <a href="https://www.statlearning.com/" class="text-accent">Introduction to Statistical Learning</a>, Chapter 6.4: Considerations in High Dimensions
          </div>
        </li>
        <li>
          W9: Can Statistical Modelling (Still) Beat Deep Learning?<br />
          <div class="text-sm ml-4">
            <a href="https://arxiv.org/abs/2207.08815" class="text-accent">Why do tree-based models still outperform deep learning on tabular data?</a>
          </div>
        </li>
        <li>
          W10: <s>Catboost</s> FT-Transformer<br/>
          <div class="text-sm ml-4">
            <a href="https://arxiv.org/pdf/2106.11959" class="text-accent">Revisiting Deep Learning Models for Tabular Data</a>
          </div>
        </li>
        <li>
          W11: Gaussian Mixture Models and the EM Algorithm<br />
          <div class="text-sm ml-4">
            <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" class="text-accent">Pattern Recognition and Machine Learning</a>, Chapter 9: Mixture Models and EM<br />
            <a href="https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf" class="text-accent">Elements of Statistical Learning</a>, Chapter 8.5: The EM Algorithm
          </div>
        </li>
        <li>
          W12: Sampling Methods: Markov Chain Monte Carlo<br />
          <div class="text-sm ml-4">
            <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" class="text-accent">Pattern Recognition and Machine Learning</a>, Chapter 11.2-3: Sampling Methods<br />
            <a href="https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf" class="text-accent">Elements of Statistical Learning</a>, Chapter 8.6: MCMC for Sampling from the Posterior
          </div>
        </li>
      </ul>
      
      <h2 class="my-4 text-2xl font-medium">Synthetic Data Generation</h2>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W13: General Comparison: GAN-based and Autoencoder-based Data Augmentation<br />
          <div class="text-sm ml-4">
            <a href="https://www.notion.so/AITU-Spring-2025-17951d7ec2c68024b7e2de2fd642da24?pvs=21" class="text-accent">Modeling Tabular Data using Conditional GAN</a>
          </div>
        </li>
      </ul>
      
      <h2 class="my-4 text-2xl font-medium">Training our own GPT</h2>
      <p class="text-lg font-light text-secondary">
        After reading in detail about GPT-1, it will be time to get our hands dirty. We will make use of Andrej Karpathy’s <a href="https://github.com/karpathy/nanoGPT" class="text-accent">open source implementation</a> of GPT-2 to train our small language model. We will go through the whole setup from collecting and preparing the data to logging the training results.
      </p>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W14: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" class="text-accent">GPT-1</a> / OpenAI / 2018: Generative Pre-Trained Transformer (Decoder-Only Architecture)
        </li>
        <li>
          W15: Live Coding: Training our own <a href="https://github.com/karpathy/nanoGPT" class="text-accent">nanoGPT</a>
        </li>
        <li>
          W16: Easter Break
        </li>
      </ul>
      
      <h2 class="my-4 text-2xl font-medium">Transformer-based Models</h2>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W17: Industry Talk (TBA)
        </li>
        <li>
          W18: <a href="https://arxiv.org/abs/2004.12832" class="text-accent">ColBERT</a> / Khattab, Zaharia / 2020: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT
        </li>
        <li>
          W19: <a href="https://arxiv.org/pdf/1910.13461" class="text-accent">BART</a> / Lewis et al. / 2019: Denoising Autoencoder
        </li>
      </ul>
    </div>
  </main>
</Layout>
