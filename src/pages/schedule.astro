---
import Layout from "../layouts/Layout.astro";
---

<Layout title="Schedule" description="Schedule for the current school year">
  <main>
    <div class="max-w-4xl mt-10">
      <a href="/" class="text-sm text-accent">Back</a>
      <h1 class="my-6 text-3xl font-medium sm:text-4xl">
        Preliminary Schedule Spring 2025
      </h1>
      
      <h2 class="my-4 text-2xl font-medium">Statistical Learning Revisited</h2>
      <p class="text-lg font-light text-secondary">
        To start off, we will conduct a grand comparison of historical classification models, such as LDA/QDA, SVMs and Decision Trees as a Kahoot quiz. We will look at their performance in different dimensionality settings and discuss their respective strong and weak sides.
      </p>
      <p class="text-lg font-light text-secondary">
        During our second meeting we will look at a paper <a href="https://x.com/GaelVaroquaux/status/1549422403889106944" class="text-accent">that received some attention on Twitter</a> (X) in 2022, a couple of months before ChatGPT was released. Does it still hold? Is there still space in the world dominated by deep neural networks for tree-based models?
      </p>
      <p class="text-lg font-light text-secondary">
        We will continue the discussion with CatBoost, and later explore additional topics like GMMs and MCMC.
      </p>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          [Maciej] W8: A Comparison of Historical Classification Models<br />
          <span class="text-sm ml-4">
            <a href="https://www.statlearning.com/" class="text-accent">Introduction to Statistical Learning</a>, Chapter 6.4: Considerations in High Dimensions
          </span>
        </li>
        <li>
          [Chris] W9: Can Statistical Modelling (Still) Beat Deep Learning?<br />
          <span class="text-sm ml-4">
            <a href="https://arxiv.org/abs/2207.08815" class="text-accent">Why do tree-based models still outperform deep learning on tabular data?</a>
          </span>
        </li>
        <li>
          W10: CatBoost<br />
          <span class="text-sm ml-4">
            <a href="https://arxiv.org/pdf/1706.09516" class="text-accent">CatBoost: unbiased boosting with categorical features</a>
          </span>
        </li>
        <li>
          [Johan] W11: Gaussian Mixture Models and the EM Algorithm<br />
          <span class="text-sm ml-4">
            <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" class="text-accent">Pattern Recognition and Machine Learning</a>, Chapter 9: Mixture Models and EM<br />
            <a href="https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf" class="text-accent">Elements of Statistical Learning</a>, Chapter 8.5: The EM Algorithm
          </span>
        </li>
        <li>
          W12: Sampling Methods: Markov Chain Monte Carlo<br />
          <span class="text-sm ml-4">
            <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" class="text-accent">Pattern Recognition and Machine Learning</a>, Chapter 11.2-3: Sampling Methods<br />
            <a href="https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf" class="text-accent">Elements of Statistical Learning</a>, Chapter 8.6: MCMC for Sampling from the Posterior
          </span>
        </li>
      </ul>
      
      <h2 class="my-4 text-2xl font-medium">Synthetic Data Generation</h2>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W13: General Comparison: GAN-based and Autoencoder-based Data Augmentation<br />
          <span class="text-sm ml-4">
            <a href="https://www.notion.so/AITU-Spring-2025-17951d7ec2c68024b7e2de2fd642da24?pvs=21" class="text-accent">Modeling Tabular Data using Conditional GAN</a>
          </span>
        </li>
      </ul>
      
      <h2 class="my-4 text-2xl font-medium">Training our own GPT</h2>
      <p class="text-lg font-light text-secondary">
        After reading in detail about GPT-1, it will be time to get our hands dirty. We will make use of Andrej Karpathyâ€™s <a href="https://github.com/karpathy/nanoGPT" class="text-accent">open source implementation</a> of GPT-2 to train our small language model. We will go through the whole setup from collecting and preparing the data to logging the training results.
      </p>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W14: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" class="text-accent">GPT-1</a> / OpenAI / 2018: Generative Pre-Trained Transformer (Decoder-Only Architecture)
        </li>
        <li>
          W15: Live Coding: Training our own <a href="https://github.com/karpathy/nanoGPT" class="text-accent">nanoGPT</a>
        </li>
        <li>
          W16: Easter Break
        </li>
      </ul>
      
      <h2 class="my-4 text-2xl font-medium">Transformer-based Models</h2>
      <ul class="my-4 text-lg font-light text-secondary">
        <li>
          W17: Industry Talk (TBA)
        </li>
        <li>
          [Maciej] W18: <a href="https://arxiv.org/abs/2004.12832" class="text-accent">ColBERT</a> / Khattab, Zaharia / 2020: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT
        </li>
        <li>
          W19: <a href="https://arxiv.org/pdf/1910.13461" class="text-accent">BART</a> / Lewis et al. / 2019: Denoising Autoencoder
        </li>
      </ul>
    </div>
  </main>
</Layout>
